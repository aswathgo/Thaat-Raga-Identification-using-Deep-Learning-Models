{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c1aae-670a-4c49-9112-a48d0d1f05db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-REQUISITES\n",
    "\n",
    "'''\n",
    "> pip install tensorflow\n",
    "> pip install matplotlib\n",
    "> pip install numpy\n",
    "> pip install scikit_learn\n",
    "\n",
    "For audio processing\n",
    "> pip install pydub\n",
    "FFMPEG for windows: https://sourceforge.net/projects/ffmpeg-windows-builds/\n",
    "\n",
    "> pip install librosa\n",
    "> pip install tqdm\n",
    "> pip install \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3b2d3e-9cb4-40f1-9922-7842b32d4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af46de2-e6f2-4dac-b098-d86491c3a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Converting to .wav from .mp3\n",
    "def mp3_to_wav(audio_path):\n",
    "    if \".mp3\" in audio_path:\n",
    "        sound = AudioSegment.from_mp3(audio_path)\n",
    "        sound.export((audio_path[:-3]+\"wav\"), format=\"wav\")\n",
    "        os.remove(audio_path)\n",
    "\n",
    "#Splitting audio into 1 minute length\n",
    "def split_audio(input_audio, input_file, output_dir, chunk_length=60):\n",
    "    y, sr = librosa.load(input_file, sr=None)\n",
    "    num_samples_per_chunk = sr * chunk_length\n",
    "\n",
    "    num_chunks = len(y) // num_samples_per_chunk + 1\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start_sample = i * num_samples_per_chunk\n",
    "        end_sample = min((i + 1) * num_samples_per_chunk, len(y))\n",
    "        chunk = y[start_sample:end_sample]\n",
    "        output_file = os.path.join(output_dir, f\"chunk_{i}_{input_audio[:-4]}.wav\")\n",
    "        sf.write(output_file, chunk, sr)\n",
    "        gc.collect()\n",
    "\n",
    "#Extracting spectrogram images\n",
    "def extractor(audio_path,output_dir,name):\n",
    "    y, sr = librosa.load(audio_path)\n",
    "    D = librosa.stft(y)\n",
    "    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "    M = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    M_db = librosa.power_to_db(M, ref=np.max)\n",
    "    chroma = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "    del y; del sr; del D; del M;\n",
    "    gc.collect()\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=3, ncols=1, sharex=True)\n",
    "\n",
    "    img1 = librosa.display.specshow(S_db, x_axis='time', y_axis='log', ax=ax[0])\n",
    "    ax[0].set(title='STFT (log scale)')\n",
    "\n",
    "    img2 = librosa.display.specshow(M_db, x_axis='time', y_axis='mel', ax=ax[1])\n",
    "    ax[1].set(title='Mel Spectrogram')\n",
    "\n",
    "    img3 = librosa.display.specshow(chroma, x_axis='time', y_axis='chroma', key='Eb:maj', ax=ax[2])\n",
    "    ax[2].set(title='Chromagram')\n",
    "\n",
    "    for ax_i in ax:\n",
    "        ax_i.label_outer()\n",
    "\n",
    "    fig.set_figwidth(20)\n",
    "    fig.set_figheight(10)\n",
    "\n",
    "    plt.subplots_adjust()\n",
    "    del S_db; del M_db; del chroma;\n",
    "\n",
    "    fig.savefig(output_dir+f\"\\{name}.png\")\n",
    "    print(\"Saved for \",audio_path)\n",
    "    plt.close()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "raw_data = [\"   \"]#RAW DATA PATH HERE\n",
    "\n",
    "for thaat in os.listdir(raw_data[0]):\n",
    "    for audio_class in tqdm(os.listdir(os.path.join(raw_data[0], thaat)), desc=thaat):\n",
    "        for audio in os.listdir(os.path.join(raw_data[0], thaat, audio_class)):\n",
    "            audio_path = os.path.join(raw_data[0], thaat, audio_class, audio)\n",
    "            try:\n",
    "                mp3_to_wav(audio_path)  #Converting to .wav\n",
    "                split_audio(audio, audio_path, raw_data)  #Splitting audio into 60 sec audios\n",
    "                os.remove(os.path.join(raw_data[0], thaat, audio_class, audio))\n",
    "            except Exception as e:\n",
    "                print(\"Error:\",e)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d15db9-f3db-4a78-afff-a4e721df49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting spectrograms\n",
    "\n",
    "data = \" \" #Data path here\n",
    "os.mkdir(data)\n",
    "for thaat in os.listdir(raw_data[0]):\n",
    "    thaat_path = os.path.join(data, thaat)\n",
    "    os.mkdir(thaat_path)\n",
    "    for audio_class in tqdm(os.listdir(os.path.join(raw_data[0], thaat)), desc=thaat):\n",
    "        spec_path = os.path.join(thaat_path, audio_class)\n",
    "        os.mkdir(spec_path)\n",
    "        for audio in os.listdir(os.path.join(raw_data[0], thaat, audio_class)):\n",
    "            audio_path = os.path.join(raw_data[0], thaat, audio_class, audio)\n",
    "            try:\n",
    "                extractor(audio_path,spec_path,audio)\n",
    "            except Exception as e:\n",
    "                print(\"Error:\",e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3129618-4030-4de0-871b-e4119a354252",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''THAAT IDENTIFICATION'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbe344d-5c75-4586-8c0c-2f02ef85513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = \"  \" #insert Data path here\n",
    "data = tf.keras.utils.image_dataset_from_directory(data_path, shuffle=True)\n",
    "\n",
    "# Scaling Data\n",
    "data = data.map(lambda x,y: (x/255, y))\n",
    "\n",
    "# Splitting Data\n",
    "train_size = int(len(data)*.8)\n",
    "val_size = int(len(data)*.1)+1\n",
    "test_size = int(len(data)*.1)+1\n",
    "\n",
    "train = data.take(train_size)\n",
    "val = data.take(val_size)\n",
    "test = data.take(test_size)\n",
    "\n",
    "#Assigning class weights\n",
    "labels = []\n",
    "for batch in train.as_numpy_iterator():\n",
    "    X1, y1 = batch\n",
    "    labels.extend(list(y1))\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(labels), y = labels)\n",
    "class_weights = dict(zip(np.unique(labels), class_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fd3d9-e84f-47c9-86ed-5d48566b366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRNN Network\n",
    "import keras.backend as K\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Dense, Flatten, Dropout, AveragePooling2D, Concatenate, GlobalAveragePooling2D, BatchNormalization, ReLU, Add, SeparableConv2D, DepthwiseConv2D, GRU, LSTM, Permute, Reshape, Activation\n",
    "\n",
    "def CRNN2D(X_shape, nb_classes):\n",
    "    nb_layers = 4\n",
    "    nb_filters = [64, 128, 128, 128]\n",
    "    kernel_size = (3, 3)\n",
    "    activation = 'elu'\n",
    "    pool_size = [(2, 2), (4, 2), (4, 2), (4, 2),\n",
    "                 (4, 2)]\n",
    "\n",
    "    input_shape = (X_shape[0], X_shape[1], X_shape[2])\n",
    "    frequency_axis = 1\n",
    "    time_axis = 2\n",
    "    channel_axis = 3\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(axis=frequency_axis, input_shape=input_shape))\n",
    "\n",
    "    model.add(Conv2D(nb_filters[0], kernel_size=kernel_size, padding='same',\n",
    "                     data_format=\"channels_last\",\n",
    "                     input_shape=input_shape))\n",
    "    model.add(BatchNormalization(axis=channel_axis))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(MaxPool2D(pool_size=pool_size[0], strides=pool_size[0]))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    for layer in range(nb_layers - 1):\n",
    "        model.add(Conv2D(nb_filters[layer + 1], kernel_size=kernel_size,\n",
    "                         padding='same'))\n",
    "        model.add(BatchNormalization(axis=channel_axis))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(MaxPool2D(pool_size=pool_size[layer + 1],\n",
    "                               strides=pool_size[layer + 1]))\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "    model.add(Permute((time_axis, frequency_axis, channel_axis)))\n",
    "    resize_shape = model.output_shape[2] * model.output_shape[3]\n",
    "    model.add(Reshape((model.output_shape[1], resize_shape)))\n",
    "\n",
    "    model.add(GRU(32, return_sequences=True))\n",
    "    model.add(GRU(32, return_sequences=False))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "input_shape = (256, 256, 3)\n",
    "n_classes = len(os.listdir(data_dir[0]))\n",
    "\n",
    "model = CRNN2D(input_shape, n_classes)\n",
    "model.compile('Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6bb702-56a3-4b14-84f6-93793ac50980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "logdir= \" \" #Insert Log path here\n",
    "tensorboard_callback1 = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "tensorboard_callback2 = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0010, patience=5, verbose=1, mode='max', restore_best_weights=True, start_from_epoch=5)\n",
    "hist = model.fit(train, class_weight=class_weights, epochs=60, validation_data=val, callbacks=[tensorboard_callback1, tensorboard_callback2])\n",
    "print(hist.history)\n",
    "\n",
    "#Save\n",
    "save_path = \" \" #Insert model folder path here\n",
    "model.save(os.path.join(save_path,'crnn2d_classifier.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce5b481-31b7-42c4-8d80-ac02e23c7827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss curve\n",
    "fig = plt.figure()\n",
    "plt.title(\"Model 1: Thaat Classifier\\n\\n\", loc='center', fontsize=26)\n",
    "plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "fig.suptitle('Loss', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# Accuracy curve\n",
    "fig = plt.figure()\n",
    "plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414fcd9a-f87f-4ecf-9dff-456fddc825d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,auc\n",
    "\n",
    "\n",
    "def metrics(labels, predictions, classes, l):\n",
    "    print(\"Classification Report:\")\n",
    "    print(\"Labels : \",labels.shape, '\\nPredictions : ', predictions.shape)\n",
    "    print(classification_report(labels, predictions, target_names = classes, digits = 4))\n",
    "    matrix = confusion_matrix(labels, predictions)\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(matrix)\n",
    "    display = ConfusionMatrixDisplay(confusion_matrix = matrix, display_labels = l)\n",
    "    display.plot()\n",
    "    plt.show()\n",
    "    print(\"\\nClasswise Accuracy :{}\".format(matrix.diagonal()/matrix.sum(axis = 1)))\n",
    "    print(\"\\nBalanced Accuracy Score: \",balanced_accuracy_score(labels,predictions))\n",
    "\n",
    "\n",
    "def plot_roc(val_label,decision_val, caption='ROC Curve'):\n",
    "    num_classes=np.unique(val_label).shape[0]\n",
    "    classes = []\n",
    "    for i in range(num_classes):\n",
    "        classes.append(i)\n",
    "    plt.figure()\n",
    "    decision_val = label_binarize(decision_val, classes=classes)\n",
    "\n",
    "    if num_classes!=2:\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(num_classes):\n",
    "            y_val = label_binarize(val_label, classes=classes)\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_val[:, i], decision_val[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        for i in range(num_classes):\n",
    "            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                           ''.format(i+1, roc_auc[i]))\n",
    "    else:\n",
    "        fpr,tpr,_ = roc_curve(val_label,decision_val, pos_label=1)\n",
    "        roc_auc = auc(fpr,tpr)*100\n",
    "        plt.plot(fpr,tpr,label='ROC curve (AUC=%0.2f)'%roc_auc)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(caption)\n",
    "    plt.legend(loc=\"lower right\",  bbox_to_anchor=(1, 0.5))\n",
    "    plt.savefig(str(len(classes))+'.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68463df-c30e-4a39-9932-ff7ae41d2623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"For CRNN2D: \")\n",
    "labels = []\n",
    "predictions = []\n",
    "for batch1 in test.as_numpy_iterator():\n",
    "    X1, y1 = batch1\n",
    "    labels.extend(list(y1))\n",
    "    yhat1 = model.predict(X1, verbose=0)\n",
    "    max = a = 0\n",
    "    arr = np.ones(yhat1.shape[0])*-1\n",
    "    for i in yhat1:\n",
    "        max = np.argmax(i)\n",
    "        arr[a] = max\n",
    "        a+=1\n",
    "    predictions.extend(list(arr))\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "predictions = np.asarray(predictions)\n",
    "\n",
    "metrics(labels, predictions, ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'])\n",
    "plot_roc(labels, predictions)\n",
    "\n",
    "avg_acc_list = []\n",
    "avg_precision_list = []\n",
    "avg_recall_list = []\n",
    "avg_f1_list = []\n",
    "\n",
    "acc_fold = accuracy_score(labels, predictions)\n",
    "avg_acc_list.append(acc_fold)\n",
    "precision_fold = precision_score(labels, predictions, average='macro')\n",
    "avg_precision_list.append(precision_fold)\n",
    "recall_fold = recall_score(labels, predictions, average='macro')\n",
    "avg_recall_list.append(recall_fold)\n",
    "f1_fold  = f1_score(labels, predictions, average='macro')\n",
    "avg_f1_list.append(f1_fold)\n",
    "\n",
    "print('Accuracy[{:.4f}] Precision[{:.4f}] Recall[{:.4f}] F1[{:.4f}] ]'.format(acc_fold, precision_fold, recall_fold, f1_fold ))\n",
    "print('________________________________________________________________')\n",
    "\n",
    "avg_acc = np.asarray(avg_acc_list)\n",
    "avg_pre = np.asarray(avg_precision_list)\n",
    "avg_recall = np.asarray(avg_recall_list)\n",
    "avg_f1 = np.asarray(avg_f1_list)\n",
    "print(\"\\n\")\n",
    "print('Overall Accuracy[{:.4f}] Overall Precision[{:.4f}] Overall Recall[{:.4f}] Overall F1[{:.4f}]'.format(np.mean(avg_acc), np.mean(avg_pre), np.mean(avg_recall), np.mean(avg_f1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757fd71-f852-4d1e-a526-6584be9bbafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a15608-fd67-4602-9ee3-f9f877b1a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' RAAG IDENTIFICATION'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58510802-7759-4d83-93c0-201065b9947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = {}\n",
    "data_iterators = {}\n",
    "batches = {}\n",
    "class_weights_r = {}\n",
    "for i in os.listdir(data_dir[0]):\n",
    "    datas[i] = tf.keras.utils.image_dataset_from_directory(os.path.join(data_dir[0], i), shuffle=True)\n",
    "    #Scaling\n",
    "    datas[i] = datas[i].map(lambda x,y: (x/255, y))\n",
    "    #Splitting\n",
    "    train_size = int(len(datas[i])*.8)\n",
    "    val_size = int(len(datas[i])*.1)+1\n",
    "    test_size = int(len(datas[i])*.1)+1\n",
    "    print(\"For \"+i+\" : \", train_size, \" + \", val_size, \" + \", test_size, \" = \", len(datas[i]))\n",
    "\n",
    "    trains[i] = datas[i].take(train_size)\n",
    "    vals[i] = datas[i].take(val_size)\n",
    "    tests[i] = datas[i].take(test_size)\n",
    "\n",
    "    #Assigning weights\n",
    "    labels = []\n",
    "    for batch in trains[i].as_numpy_iterator():\n",
    "        X1, y1 = batch\n",
    "        labels.extend(list(y1))\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    print(\"For\", i, \":\", labels)\n",
    "\n",
    "    # Calculate class weights for thaat classes\n",
    "    class_weights_r[i] = compute_class_weight(class_weight = 'balanced', classes = np.unique(labels), y = labels)\n",
    "    class_weights_r[i] = dict(zip(np.unique(labels), class_weights_r[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43709f4c-f331-4584-8ee7-457ca59da6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Network\n",
    "import keras.backend as K\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Dense, Flatten, Dropout, AveragePooling2D, Concatenate, GlobalAveragePooling2D, BatchNormalization, ReLU, Add, SeparableConv2D, DepthwiseConv2D, GRU, LSTM, Permute, Reshape, Activation\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "\n",
    "def densenet(img_shape, n_classes):\n",
    "    densenet = DenseNet201(input_shape=img_shape, include_top=False, weights='imagenet')\n",
    "    densenet.trainable = False\n",
    "\n",
    "    input = Input(img_shape)\n",
    "    x = densenet(input, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(rate = 0.2)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    output = Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(input, output)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "input_shape = (256, 256, 3)\n",
    "models = {}\n",
    "for i in os.listdir(data_dir[0]):\n",
    "    n_classes = len(os.listdir(os.path.join(data_dir[0], i)))\n",
    "    models[i] = densenet(input_shape, n_classes)\n",
    "    models[i].compile('Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    models[i].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb01fcd2-67ec-4184-9f29-bb4765163203",
   "metadata": {},
   "outputs": [],
   "source": [
    "hists = {}\n",
    "logir = \" \" #Insert log folder path here\n",
    "#Training \n",
    "for i in os.listdir(data_dir[0]):\n",
    "    print(\"\\n   For \"+i)\n",
    "    tensorboard_callback1 = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "    tensorboard_callback2 = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0010, patience=5, verbose=1, mode='max', restore_best_weights=True, start_from_epoch=5)\n",
    "    hists[i] = models[i].fit(trains[i], class_weight = class_weights_r[i], epochs=60, validation_data=vals[i], callbacks=[tensorboard_callback1, tensorboard_callback2])\n",
    "    print(hists[i].history)\n",
    "\n",
    "#Saving\n",
    "save_path = \" \" #Insert models folder path here\n",
    "for i in os.listdir(data_dir[0]):\n",
    "    filename = i+\".h5\"\n",
    "    models[i].save(os.path.join(save_path,filename)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dedf48-fb49-453f-8ed7-6f9fff66bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "for i in os.listdir(data_dir[0]):\n",
    "    title = \"Model \"+str(a)+\": \"+i+\"\\n\"\n",
    "    # Loss curve\n",
    "    fig = plt.figure()\n",
    "    plt.title(title, loc='center', fontsize=26)\n",
    "    plt.plot(hists[i].history['loss'], color='teal', label='loss')\n",
    "    plt.plot(hists[i].history['val_loss'], color='orange', label='val_loss')\n",
    "    fig.suptitle('Loss', fontsize=20)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy curve\n",
    "    fig = plt.figure()\n",
    "    plt.plot(hists[i].history['accuracy'], color='teal', label='accuracy')\n",
    "    plt.plot(hists[i].history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "    fig.suptitle('Accuracy', fontsize=20)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    a+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35187134-8f4d-4487-b01d-8dc3696eb70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in os.listdir(data_dir[0]):\n",
    "    print(\"For \"+i+\": \")\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for batch1 in tests[i].as_numpy_iterator():\n",
    "        X1, y1 = batch1\n",
    "        labels.extend(list(y1))\n",
    "        yhat1 = models[i].predict(X1, verbose=0)\n",
    "        max = a = 0\n",
    "        arr = np.ones(yhat1.shape[0])*-1\n",
    "        for i in yhat1:\n",
    "            max = np.argmax(i)\n",
    "            arr[a] = max\n",
    "            a+=1\n",
    "        predictions.extend(list(arr))\n",
    "\n",
    "    labels = np.asarray(labels)\n",
    "    predictions = np.asarray(predictions)\n",
    "\n",
    "    classes = [z for z, p in enumerate(os.listdir(os.path.join(data_dir[0], i)))]\n",
    "    classes = np.array(classes)\n",
    "    classes+=1\n",
    "\n",
    "    #Function defined earlier\n",
    "    metrics(labels, predictions, classes)\n",
    "    plot_roc(labels, predictions)\n",
    "\n",
    "\n",
    "    avg_acc_list = []\n",
    "    avg_precision_list = []\n",
    "    avg_recall_list = []\n",
    "    avg_f1_list = []\n",
    "\n",
    "    acc_fold = accuracy_score(labels, predictions)\n",
    "    avg_acc_list.append(acc_fold)\n",
    "    precision_fold = precision_score(labels, predictions, average='macro')\n",
    "    avg_precision_list.append(precision_fold)\n",
    "    recall_fold = recall_score(labels, predictions, average='macro')\n",
    "    avg_recall_list.append(recall_fold)\n",
    "    f1_fold  = f1_score(labels, predictions, average='macro')\n",
    "    avg_f1_list.append(f1_fold)\n",
    "\n",
    "    print('Accuracy[{:.4f}] Precision[{:.4f}] Recall[{:.4f}] F1[{:.4f}] ]'.format(acc_fold, precision_fold, recall_fold, f1_fold ))\n",
    "    print('________________________________________________________________')\n",
    "\n",
    "    avg_acc = np.asarray(avg_acc_list)\n",
    "    avg_pre = np.asarray(avg_precision_list)\n",
    "    avg_recall = np.asarray(avg_recall_list)\n",
    "    avg_f1 = np.asarray(avg_f1_list)\n",
    "    print(\"\\n\")\n",
    "    print('Overall Accuracy[{:.4f}] Overall Precision[{:.4f}] Overall Recall[{:.4f}] Overall F1[{:.4f}]'.format(np.mean(avg_acc), np.mean(avg_pre), np.mean(avg_recall), np.mean(avg_f1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
